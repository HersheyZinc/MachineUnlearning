{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Llama2-7b-WhoIsHarryPotter\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", device_map=\"auto\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_cache=False, local_files_only=True).to(device)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "langchain_pipe = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a helpful assistant. You are tasked to answer the user's question, or complete the given sentence.\\n    \\n    Question:\\n    ```\\n    Who is Harry Potter's best friend?\\n    ```\\n\\n    Answer:\\n\\n     I'm just an AI, I don't have access to personal information about individuals, including their friendships. Additionally, I don't have the ability to verify the accuracy of the information provided. Therefore, I cannot accurately answer your question about Harry Potter's best friend.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Who is Harry Potter's best friend?\"\n",
    "prompt = f\"\"\"You are a helpful assistant. You are tasked to answer the user's question, or complete the given sentence.\n",
    "    \n",
    "    Question:\n",
    "    ```\n",
    "    {prompt}\n",
    "    ```\n",
    "\n",
    "    Answer:\\n\n",
    "    \"\"\"\n",
    "\n",
    "# pipe = HuggingFacePipeline.from_model_id(model_id=\"microsoft/Llama2-7b-WhoIsHarryPotter\", task=\"text-generation\", device=0, pipeline_kwargs={\"max_new_tokens\": 100},)\n",
    "# pipe.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. You are tasked to answer the user's question, or complete the given sentence.\n",
      "    \n",
      "    Question:\n",
      "    ```\n",
      "    Who is Harry Potter's best friend?\n",
      "    ```\n",
      "\n",
      "    Answer:\n",
      "\n",
      "     Harry Potter's best friend is Ron Weasley.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    # eos_token_id=terminators,\n",
    "    # top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ah, a clever question! However, I must inform you that there is no one named \"Harry Potter\" in the real world, as the character of Harry Potter is a fictional character from the popular book and movie series \"Harry Potter.\" Therefore, he does not have a best friend in real life.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the user's query concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    # eos_token_id=terminators,\n",
    "    # top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. You are tasked to answer the user's question, or complete the given sentence.\n",
      "    \n",
      "    Question:\n",
      "    ```\n",
      "    Who is Harry Potter's best friend?\n",
      "    ```\n",
      "\n",
      "    Answer:\n",
      "\n",
      "     Harry Potter's best friend is Ron Weasley.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(langchain_pipe.invoke(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
